---
title: "Class07: Machine Learning1"
author: "Juan Manuel Tzompantzi De Ita (PID:A69034758)"
format: pdf
---

Before we get into clustering methods let's make some sample data to cluster where we know what the answer should be... 

To help with this we will use `rnom()` fucntion. 

```{r}
rnorm(10)


```

```{r}
p <- hist( rnorm(150000, mean=3))
```


```{r}
hist( rnorm(150000, mean=-3))
```
To plot both data sets, we can vectorized the data. 
```{r}
n = 30
hist( c( rnorm(n, mean=3), rnorm(n, mean=-3) ))
```

Let's say we wanna create a multidimentional data set: 

We can use the same distribution but apply different rules for the different axes. 

```{r}
n = 30
x <- c( rnorm(n, mean=3), rnorm(n, mean=-3) )
y <- rev(x)


z <- cbind(x,y)
plot(z)
```

## K-means clustering

The function in base R for k-means clustering is called `kmeans()`
```{r}
?kmeans()
```

```{r}
kmeans(z, centers=2)
```
If we wanna know that the components are we can rewrite it: 
```{r}
km <- kmeans(z, centers=2)
km
```
And then explore the components with: 
```{r}
km$centers
```

>Q. Print out the cluster membership vector (i.e. our main answer)

```{r}
km$cluster
```
If we wanna color out plots, this command will give us a plot that iterates between colors...
```{r}

plot(z, col=c("red","blue"))
```
 Same with this one: 
```{r}
plot(z, col=c(1,2))
```
 
 We need to do this: 
```{r}
plot(z, col=km$cluster)
```

When, plot with clustering results and add cluster centers: 
```{r}
plot(z, col=km$cluster)
points(km$centers, col ="blue", pch=15, cex=2)
```

>Q. Can you cluster our data in z into four clusters? 

```{r}
km4 <- kmeans(z, centers=4)
km4
```
```{r}
plot(z, col=km4$cluster)
points(km4$centers, col ="blue", pch=15, cex=2)
```

##Hierarchical Clustering

The main function for hierarchical clustering `hclust()`

Unlike `kmeans()`, I cannot just pass in my data as input. I first need a distance matrix from my data. 

```{r}
d <- dist(z)
hc <- hclust(d)
hc
```

There is a specific hclust plot for this method. 
```{r}
plot(hc)
abline(h=10, col="red")
```

To get my main clustering result (i.e. the membership vector), I can "cut" my tree at a given height. To do this I will use the: `cutree()`:

```{r}
grps <- cutree(hc, h=10)
grps
```

```{r}
plot(z, col=grps)
```

##Principal Component Analysis 
PCA prjects the features onto the principal components. The motivation is to decude the futures dimensionality while only losing a small amount of information. 

Principal component analysis (PCA) is a well established "multivariate statistical technique" used to reduce the dimensionality of a complex data set to a more manageable number (typically 2D or 3D). This method is particularly useful for highlighting strong paterns and relationships in large datasets (i.e. revealing major similarities and diferences) that are otherwise hard to visualize.

Hands-on practice!

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1) #eliminating the first column 

x
```
```{r}
dim(x)
```
```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
```

Trying to understand the main differences with a lot of plots...

```{r}
pairs(x, col=rainbow(10), pch=16)
```

As we can see, spotting main differences is hard even for only 4 countries and 17 dimensions.. 
We are gonna be using the PCA.
The main function of R base for PCA is `prcomp()`

```{r}
pca <- prcomp( t(x) ) #Changing order of columns and rows. 
summary(pca)
```
For the summary above, we can tell that PC1, PC2 and PC3 are sufficient enough to explain our data since they contribute to almost all the variance of our 17 dimensions. 

To see what is inside of our result object `pca` that we just created
```{r}
attributes(pca)
```

To make the main result figure, called a "PC plot" (or "score plot", "ordination plot" or "PC1 vs PC2 plot")

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1 (67.4%)", ylab="PC2 (29%)")
text(pca$x[,1], pca$x[,2], colnames(x), col=c("black","red","blue","darkgreen"))
abline(v=0, col="grey", lty=2)
abline(h=0, col="grey", lty=2)
```

#Variable loading plots 
Digging deeper into the data. Since we see that PC1 component gives most of the variance, we can plot this component and see what the data looks like here. 

```{r}
par(mar=c(10,3,0.35,0))
barplot( pca$rotation[,1],las=2)
```


